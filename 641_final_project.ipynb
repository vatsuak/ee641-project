{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7781d076713446ffb72391019b869388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd751c8635ab49bfa8866ce7d9890661",
              "IPY_MODEL_525999ba26074e9f8aa92ea099751680",
              "IPY_MODEL_e0fb15c52aa24db997fed3f2bc7cfe73"
            ],
            "layout": "IPY_MODEL_dc32d405959b4cb5b0eea991724df2d5"
          }
        },
        "cd751c8635ab49bfa8866ce7d9890661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0791405ece74de2943e0a9594f83ef9",
            "placeholder": "​",
            "style": "IPY_MODEL_d757761b542845df90add19bbb294484",
            "value": " 20%"
          }
        },
        "525999ba26074e9f8aa92ea099751680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8252d0d69114f88a180a3cdd0e4a21c",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_556dd68ffcbd411d8eca0366c978a26a",
            "value": 49
          }
        },
        "e0fb15c52aa24db997fed3f2bc7cfe73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79a70e89e5a04495940dce59a0079d39",
            "placeholder": "​",
            "style": "IPY_MODEL_b5ea83c3f75a450ba9d6972fa2181753",
            "value": " 49/250 [00:36&lt;01:57,  1.72it/s]"
          }
        },
        "dc32d405959b4cb5b0eea991724df2d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0791405ece74de2943e0a9594f83ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d757761b542845df90add19bbb294484": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8252d0d69114f88a180a3cdd0e4a21c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556dd68ffcbd411d8eca0366c978a26a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79a70e89e5a04495940dce59a0079d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ea83c3f75a450ba9d6972fa2181753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d66_xxoUWyRK",
        "outputId": "295586f9-da2a-453f-9eb5-fdae103c878f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = \"drive/MyDrive/641\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVoqCaeJX_6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator"
      ],
      "metadata": {
        "id": "LNMsDocEW6oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features), \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, 3),\n",
        "            nn.InstanceNorm2d(in_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "class GeneratorResNet(nn.Module):\n",
        "    def __init__(self, input_shape, num_residual_block):\n",
        "        super(GeneratorResNet, self).__init__()\n",
        "        \n",
        "        channels = input_shape[0]\n",
        "        \n",
        "        # Initial Convolution Block\n",
        "        out_features = 64\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(channels),\n",
        "            nn.Conv2d(channels, out_features, 7),\n",
        "            nn.InstanceNorm2d(out_features),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        in_features = out_features\n",
        "        \n",
        "        # Downsampling\n",
        "        for _ in range(2):\n",
        "            out_features *= 2\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "        \n",
        "        # Residual blocks\n",
        "        for _ in range(num_residual_block):\n",
        "            model += [ResidualBlock(out_features)]\n",
        "            \n",
        "        # Upsampling\n",
        "        for _ in range(2):\n",
        "            out_features //= 2\n",
        "            model += [\n",
        "                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n",
        "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            \n",
        "        # Output Layer\n",
        "        model += [nn.ReflectionPad2d(channels),\n",
        "                  nn.Conv2d(out_features, channels, 7),\n",
        "                  nn.Tanh()\n",
        "                 ]\n",
        "        \n",
        "        # Unpacking\n",
        "        self.model = nn.Sequential(*model) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "VpvoXRnkW3hB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrimator\n"
      ],
      "metadata": {
        "id": "xdiU2tLNW56k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        channels, height, width = input_shape\n",
        "        \n",
        "        # Calculate output shape of image discriminator (PatchGAN)\n",
        "        self.output_shape = (1, height//2**4, width//2**4)\n",
        "        \n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.InstanceNorm2d(out_filters))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            *discriminator_block(channels, 64, normalize=False),\n",
        "            *discriminator_block(64, 128),\n",
        "            *discriminator_block(128,256),\n",
        "            *discriminator_block(256,512),\n",
        "            nn.ZeroPad2d((1,0,1,0)),\n",
        "            nn.Conv2d(512, 1, 4, padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ],
      "metadata": {
        "id": "6rO16BZ-XEUn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Losses"
      ],
      "metadata": {
        "id": "De2RU9CFXT54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_GAN = torch.nn.MSELoss()\n",
        "criterion_cycle = torch.nn.L1Loss()\n",
        "criterion_identity = torch.nn.L1Loss()"
      ],
      "metadata": {
        "id": "5k-H5GH5XUGc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models/optimizers"
      ],
      "metadata": {
        "id": "qjh7q4ahYWWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# data (img)\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "channels = 3\n",
        "\n",
        "n_cpu = 2 # number of cpu threads to use during batch generation\n",
        "\n",
        "# training\n",
        "epoch = 0 # epoch to start training from\n",
        "n_epochs = 5 # number of epochs of training\n",
        "batch_size = 1 # size of the batches\n",
        "lr = 0.0002 # adam : learning rate\n",
        "b1 = 0.5 # adam : decay of first order momentum of gradient\n",
        "b2 = 0.999 # adam : decay of first order momentum of gradient\n",
        "decay_epoch = 3 # suggested default : 100 (suggested 'n_epochs' is 200)\n",
        "                 # epoch from which to start lr decay\n"
      ],
      "metadata": {
        "id": "jRmE9Hny4rI7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (channels, img_height, img_width) # (3,256,256)\n",
        "n_residual_blocks = 9 # suggested default, number of residual blocks in generator\n",
        "\n",
        "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
        "D_A = Discriminator(input_shape)\n",
        "D_B = Discriminator(input_shape)\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "if cuda:\n",
        "    G_AB = G_AB.cuda()\n",
        "    G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    \n",
        "    criterion_GAN.cuda()\n",
        "    criterion_cycle.cuda()\n",
        "    criterion_identity.cuda()"
      ],
      "metadata": {
        "id": "nhpuOdavXY0u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "# lr = 0.0002\n",
        "# b1 = 0.5\n",
        "# b2 = 0.999\n",
        "\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
        ")\n",
        "\n",
        "optimizer_D_A = torch.optim.Adam(\n",
        "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
        ")\n",
        "optimizer_D_B = torch.optim.Adam(\n",
        "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
        ")"
      ],
      "metadata": {
        "id": "PxK2WHfTXh1C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA"
      ],
      "metadata": {
        "id": "HMclGxmtYUTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transforms_ = [\n",
        "    transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n",
        "    transforms.RandomCrop((img_height, img_width)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]"
      ],
      "metadata": {
        "id": "Qy08jRcxXq00"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LambdaLR:\n",
        "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
        "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
        "        self.n_epochs = n_epochs\n",
        "        self.offset = offset\n",
        "        self.decay_start_epoch = decay_start_epoch\n",
        "        \n",
        "    def step(self, epoch):\n",
        "        return 1.0 - max(0, epoch+self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
        "\n",
        "# n_epochs = 10\n",
        "# epoch = 0\n",
        "# decay_epoch = 5\n",
        "\n",
        "\n",
        "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_G,\n",
        "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")\n",
        "\n",
        "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_A,\n",
        "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")\n",
        "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_D_B,\n",
        "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
        ")"
      ],
      "metadata": {
        "id": "GLsVyhy7Xrr3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_rgb(image):\n",
        "    rgb_image = Image.new(\"RGB\", image.size)\n",
        "    rgb_image.paste(image)\n",
        "    return rgb_image"
      ],
      "metadata": {
        "id": "HzSKz71QXryX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
        "        self.transform = transforms.Compose(transforms_)\n",
        "        self.unaligned = unaligned\n",
        "        self.mode = mode\n",
        "        if self.mode == 'train':\n",
        "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[:250])\n",
        "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[:250])\n",
        "        elif self.mode == 'test':\n",
        "            self.files_A = sorted(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*')[250:])\n",
        "            self.files_B = sorted(glob.glob(os.path.join(root+'/photo_jpg')+'/*.*')[250:301])\n",
        "\n",
        "    def  __getitem__(self, index):\n",
        "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
        "        \n",
        "        if self.unaligned:\n",
        "            image_B = Image.open(self.files_B[np.random.randint(0, len(self.files_B)-1)])\n",
        "        else:\n",
        "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
        "        if image_A.mode != 'RGB':\n",
        "            image_A = to_rgb(image_A)\n",
        "        if image_B.mode != 'RGB':\n",
        "            image_B = to_rgb(image_B)\n",
        "            \n",
        "        item_A = self.transform(image_A)\n",
        "        item_B = self.transform(image_B)\n",
        "        return {'A':item_A, 'B':item_B}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return max(len(self.files_A), len(self.files_B))\n",
        "            "
      ],
      "metadata": {
        "id": "fzN3vnBeYA-l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(root+'/monet_jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv5GB1L65nBa",
        "outputId": "5b441cb3-9ca1-4841-824e-6122e5d0c9d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/641/monet_jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(glob.glob(os.path.join(root+'/monet_jpg')+'/*.*'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV6IQJ3D5knz",
        "outputId": "cd65c2e9-8672-4965-9ff6-7e4b7c6de39c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(\n",
        "    ImageDataset(root, transforms_=transforms_, unaligned=True),\n",
        "    batch_size=1, # 1\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu # 3\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(root, transforms_=transforms_, unaligned=True, mode='test'),\n",
        "    batch_size=5,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu\n",
        ")"
      ],
      "metadata": {
        "id": "zyIlkqsDYGQ-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "q-nVADuzYRiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoch, n_epochs):\n",
        "    for i, batch in enumerate(tqdm(dataloader)):\n",
        "        \n",
        "        # Set model input\n",
        "        real_A = batch['A'].type(Tensor)\n",
        "        real_B = batch['B'].type(Tensor)\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
        "        fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
        "        \n",
        "# -----------------\n",
        "# Train Generators\n",
        "# -----------------\n",
        "        G_AB.train() # train mode\n",
        "        G_BA.train() # train mode\n",
        "        \n",
        "        optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)\n",
        "        \n",
        "        # Identity Loss\n",
        "        loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,\n",
        "        loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.\n",
        "                                                             # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').\n",
        "        loss_identity = (loss_id_A + loss_id_B)/2\n",
        "        \n",
        "        # GAN Loss\n",
        "        fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing\n",
        "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'\n",
        "        fake_A = G_BA(real_B)\n",
        "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'\n",
        "        \n",
        "        loss_GAN = (loss_GAN_AB + loss_GAN_BA)/2\n",
        "        \n",
        "        # Cycle Loss\n",
        "        recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo\n",
        "        loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image\n",
        "        recov_B = G_AB(fake_A)\n",
        "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
        "        \n",
        "        loss_cycle = (loss_cycle_A + loss_cycle_B)/2\n",
        "        \n",
        "# ------> Total Loss\n",
        "        loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)\n",
        "        \n",
        "        loss_G.backward()\n",
        "        optimizer_G.step()\n",
        "        \n",
        "# -----------------\n",
        "# Train Discriminator A\n",
        "# -----------------\n",
        "        optimizer_D_A.zero_grad()\n",
        "    \n",
        "        loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real\n",
        "        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake\n",
        "        \n",
        "        loss_D_A = (loss_real + loss_fake)/2\n",
        "        \n",
        "        loss_D_A.backward()\n",
        "        optimizer_D_A.step()\n",
        "\n",
        "# -----------------\n",
        "# Train Discriminator B\n",
        "# -----------------\n",
        "        optimizer_D_B.zero_grad()\n",
        "    \n",
        "        loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real\n",
        "        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake\n",
        "        \n",
        "        loss_D_B = (loss_real + loss_fake)/2\n",
        "        \n",
        "        loss_D_B.backward()\n",
        "        optimizer_D_B.step()\n",
        "        \n",
        "# ------> Total Loss\n",
        "        loss_D = (loss_D_A + loss_D_B)/2\n",
        "    \n",
        "# -----------------\n",
        "# Show Progress\n",
        "# -----------------\n",
        "        if (i+1) % 50 == 0:\n",
        "            sample_images()\n",
        "            print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n",
        "                    %(epoch+1,n_epochs,       # [Epoch -]\n",
        "                      i+1,len(dataloader),   # [Batch -]\n",
        "                      loss_D.item(),       # [D loss -]\n",
        "                      loss_G.item(),       # [G loss -]\n",
        "                      loss_GAN.item(),     # [adv -]\n",
        "                      loss_cycle.item(),   # [cycle -]\n",
        "                      loss_identity.item(),# [identity -]\n",
        "                     ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "7781d076713446ffb72391019b869388",
            "cd751c8635ab49bfa8866ce7d9890661",
            "525999ba26074e9f8aa92ea099751680",
            "e0fb15c52aa24db997fed3f2bc7cfe73",
            "dc32d405959b4cb5b0eea991724df2d5",
            "b0791405ece74de2943e0a9594f83ef9",
            "d757761b542845df90add19bbb294484",
            "c8252d0d69114f88a180a3cdd0e4a21c",
            "556dd68ffcbd411d8eca0366c978a26a",
            "79a70e89e5a04495940dce59a0079d39",
            "b5ea83c3f75a450ba9d6972fa2181753"
          ]
        },
        "id": "etIlNfNiYQgS",
        "outputId": "3f5907c6-1d3d-4b54-e22b-d1861b19b90a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7781d076713446ffb72391019b869388"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d1d739898086>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n\u001b[1;32m     83\u001b[0m                     %(epoch+1,n_epochs,       # [Epoch -]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_images' is not defined"
          ]
        }
      ]
    }
  ]
}